\documentclass[14pt]{extarticle}
\usepackage[]{cite}
\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}



\usepackage{amsmath, amsfonts,amssymb,mathrsfs}
\usepackage{graphicx, epsfig}
\usepackage{subfig}
\usepackage{color}

\usepackage{wrapfig}
\usepackage{float}
\usepackage{subfloat}
\usepackage{caption}
\usepackage{multirow}
\graphicspath{{../pics/}}

\newcommand\argmin{\mathop{\arg\min}}
\newcommand{\T}{^{\text{\tiny\sffamily\upshape\mdseries T}}}
\newcommand{\hchi}{\hat{\boldsymbol{\chi}}}
\newcommand{\hphi}{\hat{\boldsymbol{\varphi}}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\hx}{\hat{x}}
\newcommand{\hy}{\hat{y}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\p}{p(\cdot)}
\newcommand{\q}{q(\cdot)}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}


\renewcommand{\baselinestretch}{1}


\newtheorem{Th}{Теорема}
\newtheorem{Def}{Определение}
\newenvironment{Proof} % имя окружения
    {\par\noindent{\bf Доказательство.}} % команды для \begin
    {\hfill$\scriptstyle\blacksquare$} % команды для \end
\newtheorem{Assumption}{Предположение}
\newtheorem{Corollary}{Следствие}

\textheight=22cm % высота текста
\textwidth=16cm % ширина текста
\oddsidemargin=0pt % отступ от левого края
\topmargin=-1.5cm % отступ от верхнего края
\parindent=24pt % абзацный отступ
\parskip=5pt % интервал между абзацами
\tolerance=2000 % терпимость к "жидким" строкам
\flushbottom % выравнивание высоты страниц

%\graphicspath{ {fig/} }



\begin{document}

\thispagestyle{empty}
\begin{center}
    \sc
        «Московский физико-технический институт \rm{(национальный исследовательский университет)}»\\
        Физтех-школа прикладной математики и информатики\\
        Кафедра <<Интеллектуальные системы>>
        %\\        при Вычислительном центре им. А. А. Дородницына РАН
        \\[35mm]
    \rm\large
        Северилов Павел Андреевич\\[10mm]
    \bf\Large
		Оценка качества прогнозирования структуры белка с использованием графовых свёрточных нейронных сетей\\[10mm]
    \rm\normalsize
        03.03.01 -- Прикладные математика и физика\\[10mm]
    \sc
        Выпускная квалификационная работа бакалавра\\[10mm]
\end{center}
\hfill\parbox{85mm}{
    \begin{flushleft}
    \bf
        Научный руководитель:\\
    \rm
        д.ф.-м.н. Стрижов Вадим Викторович\\[3.9cm]
    \end{flushleft}
}
\begin{center}
    Москва\\
    2020
\end{center}


\newpage
\tableofcontents
\newpage

\begin{abstract}
	Последовательность аминокислот сворачивается в нативную структуру белка. Моделируется структура, в которую произойдет сворачивание. Определить качество смоделированной структуры по отношению к нативной вычислительно дорого. В работе решается задача оценки качества структуры смоделированного белка (Quality Assessment), т.е. строится регрессия смоделированных белковых структур на значение метрики схожести её и нативной структуры белка $\text{CAD}_\text{score}$. В данной работе впервые исследуется графовый подход к задаче вместе с использованием преобразования свёртки. Преимущество предложенного подхода по сравнению с ранее представленными заключается в том, что графовое представление позволяет одновременно учитывать и первичную, и третичную структуры белка. В работе методами спектральной теории графов проанализирован спектр графовой свёртки и применены графовые свёрточные нейронные сети к задаче Quality Assessment. Эксперименты проводятся на данных с соревнований CASP по решению данной задачи, которые представляют собой трёхмерные координаты и химические свойства атомов белка. Построены графовые представления для смоделированных структур в виде матриц смежности и матриц координат атомов белков. Параметры нейросети оптимизируются на наборах данных CASP9-CASP11. Проведен анализ корреляций Пирсона и Спирмена предсказаний модели и истинных значений качества структуры на данных CASP12. Качество, достигаемое моделью, сравнимо с качеством альтернативных моделей, дающих наилучшее качество в задаче.
	
  %Решается задача оценки качества (QA -- Quality Assessment) прогнозирования белковых структур. В работе показывается применимость к рассматриваемой задаче графовых свёрточных нейронных сетей, основанных на спектральной теории. Описание белковых структур представляется в виде графов. Спектральная теория графов определяет свёртку в нейронных сетях. Нейросеть в работе получает на вход матрицы координат атомов и матрицы смежности смоделированных белковых структур. Она предсказывает близость смоделированной и реальной структуры белка в виде $\text{CAD}_\text{score}$. Нейросеть обучается на наборах данных CASP9-CASP11 и тестируется на данных CASP12. На CASP12 достигается уровень ошибки MSE равный 0.051. Дополнительный анализ корреляционных коэффициентов Пирсона и Спирмена подтверждает применимость метода для различных белковых структур. Эксперименты в данной работе показывают новые направления в задаче QA.

  \bigskip
  \textbf{Ключевые слова}: \emph{белковые структуры, графы, графовые свёртки, графовые нейронные сети, свёрточные нейронные сети, спектральный анализ.}
\end{abstract}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Введение}
%\addcontentsline{toc}{section}{\protect\numberline{}Введение}

%-----------------------------------------------------------------------------------------------------
\section*{Введение}
\addcontentsline{toc}{section}{\protect\numberline{}Введение}
\label{sec:intro}
	
	Белки являются наиболее универсальными макромолекулами в живых системах и выполняют важнейшие функции практически во всех биологических процессах~\cite{berg2002biochemistry}. 
	%Белки спонтанным образом принимают форму в различных средах [?] --  
	Форма белковой структуры определяет выполняемые ей функции~\cite{berg2002biochemistry}. Понимание белковых структур и выполняемых ими задач имеют важное значение для медицинских, фармацевтических и генетических исследований~\cite{Baldassarre2019GraphQAPM}. Решение задачи определения, в какую \textit{нативную структуру} свернётся последовательность аминокислот в белке, занимает большое количество времени и ресурсов.

Каждые два года проводятся соревнования Critical Assessment of protein Structure Prediction (CASP~\cite{CASP}) по решению задачи прогнозирования структуры. Вычислительные методы, которые её решают состоят из двух этапов: моделирование структуры белка из их аминокислотных последовательностей и оценивание качества прогнозирования. В данной работе рассматривается только второй этап. Под качеством прогнозирования понимается численное значение близости \textit{смоделированной} и нативной структур (например, метрики $\text{CAD}_\text{score}$~\cite{Olechnovic2013CADscoreAN}, LDDT~\cite{Mariani2013lDDTAL}, GDT~\cite{GDT}). Вычислять напрямую данные метрики вычислительно дорого, поэтому данная проблема рассматривается как отдельная задача.
\begin{figure}[H]
	\centering
	\subfloat[True T0861]{\label{fig:edge-a}
		\includegraphics[scale=0.32]{target_T0861.pdf}}
	\subfloat[Predicted Atome2\_CBS\_TS4]{\label{fig:edge-b}
		\includegraphics[scale=0.32]{model_T0861.pdf}}
	\captionof{figure}{Пример нативной и смоделированной структуры белка}
	\label{fig:edge}
\end{figure}
Белковая структура состоит из одной или нескольких цепочек более мелких молекул~-- аминокислотных остатков. Последовательность остатков S = $\{a_i\}_{i=1}^N$ представляет его первичную структуру, где $a_i$ является одним из 22 типов аминокислот. Взаимодействия между соседними остатками и окружающей средой определяют, как цепочка будет сворачиваться в сложные структуры, которые представляют вторичную структуру и третичную структуру белка~\cite{Baldassarre2019GraphQAPM}.

Поэтому для задач прогнозирования и оценки качества белковых структур требуется учитывать как пространственную информацию об атомах, третичную структуру, так и признаки в виде последовательностей аминокислот, первичную структуру белка.  В работах~\cite{HurtadoQA, AngularQA} для оценки качества прогнозирования белковых структур используются LSTM или 1D-CNN, которые представляют белки в виде последовательности аминокислот с пространственными признаками.  В работах~\cite{3DCNN, 10.1093/bioinformatics/btz122} прогнозируется качество структуры белков с использованием 3D-CNN, но не учитывается первичная структура белка. В данных работах не учитываются одновременно первичная и третичная структуры белка. На основе графового представления учитываются как последовательности аминокислот, так и пространственные, геометрические структуры белков. 

\begin{figure}[t]
	\centering
	%\includegraphics[width=1.02\textwidth]{experiment.pdf}
	\includegraphics[width=0.91\textwidth]{main_slide.pdf}
	\caption{Диаграмма оценки качества структуры белка}
	\label{fig:experiment}
\end{figure}
Работа~\cite{Baldassarre2019GraphQAPM}~-- единственная, в которой используется графовое представление структуры белка для решения задачи оценки качества прогнозирования структуры. В ней графовые нейронные сети на основе алгоритма, описанного в~\cite{Battaglia2018RelationalIB}, показывают результаты, превосходящие остальные современные методы, дающие наилучшее качество в задаче. В модели из~\cite{Baldassarre2019GraphQAPM} не используются свёртки. Основные результаты в задаче оценивания качества структуры белка полагаются на свёрточные нейронные сети~\cite{10.1093/bioinformatics/btz122}. 

В данной работе впервые исследуются графовые свёртки применительно к задаче Quality Assessment. Методами спектральной теории графов определена свёртка на графах и проанализирован её спектр. На основе полученного преобразования графовой свёртки построена модель графовой свёрточной нейронной сети для задачи оценки качества прогнозирования структуры белка и протестирована на данных CASP9-12. Данные для экспериментов брались с соревнований CASP прошлых лет, которые представлены в виде информации об атомах и их пространственном расположении в виде координат для нативных и смоделированных структур белков. По этим данным построено графовое представление каждой смоделированной структуры~-- матрица координат $\mathbf{X}$ и матрица смежности $\mathbf{A}$. На рисунке~\ref{fig:experiment} представлена диаграмма решения задачи оценки качества структуры белка.
%Кроме того, графы инвариантны к поворотам и сдвигам, может получать на вход белки разных размеров. 
%До недавнего времени лучшими методами предсказания стурктуры считались[?...?] объединение подходов, основанных на функциях, предназначенных для узкого класса белков. Методы глубинного обучения превзошли \cite{AlphaFold} эти результаты.
%Т.к. имеющиеся данные представляют собой трехмерные координаты атомов, то предлагается использовать графовые архитектуры нейронных сетей в комбинации с уже имеющимися архитектурами.



%-----------------------------------------------------------------------------------------------------
\section{Постановка задачи оценки качества структуры белка}

Дана выборка $$\mathfrak{D} = \left\{\mathbf{x}_i, {y}_i\right\}_{i=1}^m,$$ где $\mathbf{x}_i\in \mathbb{R}^{n_i\times 3}$~-- молекулы, каждая из которых описана множеством трёхмерных координат всех ее $n_i$ атомов, ${y}_i \in \mathbb{R}$~-- оценка близости смоделированной и нативной структуры белка. Оценка близости измеряется различными метриками: $\text{CAD}_\text{score}$~\cite{Olechnovic2013CADscoreAN}, LDDT~\cite{Mariani2013lDDTAL}, GDT~\cite{GDT}. В данной работе выбран $\text{CAD}_\text{score}$. 

%-----------------------------------------------------------------------------------------------------
\subsection{Вычисление CAD score}
%\begin{wrapfigure}{r}{0.32\textwidth}
%	\centering
%	\includegraphics[width=0.3\textwidth]{T0861_Atome2_CBS_TS4.pdf}
%	\caption{Пересечение реальной и смоделированной структур}
%	\label{CAD_example}
%\end{wrapfigure}

Обозначим через $P$ множество всех пар элементов последовательности аминокислот (остатков)  $(i, j)$, имеющих ненулевую площадь контакта $N_{(i, j)}$ в нативной структуре. Затем для каждой пары остатков~$(i, j) \in P$ вычисляется площадь контакта $M_{(i, j)}$ смоделированной структуры. 

\begin{figure}[h]
	\centering
	\includegraphics[width=0.3\textwidth]{T0861_Atome2_CBS_TS4.pdf}
	\caption{Пересечение нативной и смоделированной структур}
	\label{CAD_example}
\end{figure}

Для каждой пары остатков~$(i, j) \in P$ определяется разность площадей контакта $\mathrm{CAD}_{(i, j)}$ как абсолютная разница площадей контакта между остатками $i$ и $j$ в нативной $N$ и смоделированной структуре $M$:
$$\mathrm{CAD}_{(i, j)}=\left|N_{(i, j)}-M_{(i, j)}\right|.$$

Для вычислительной стабильности берется ограниченный CAD: $\mathrm{CAD}_{(i, j)}^{\text {bounded}}=\min \left(\mathrm{CAD}_{(i, j)}, N_{(i, j)}\right)$. Таким образом, $\text{CAD}_\text{score}$ для всей структуры определяется как
\begin{align}
\mathrm{CAD}_\text{score}=1-\cfrac{\sum_{(i, j) \in P} \mathrm{CAD}_{(i, j)}^{\text {bounded }}}{\sum_{(i, j) \in P} N_{(i, j)}}.
\label{CAD_score}
\end{align}

На рисунке~\ref{CAD_example} представлен пример пересечения нативной структуры T0861 (жёлтый) и её модели Atome2\_CBS\_TS4 (зелёный) при $\mathrm{CAD}_\text{score}=0.829$.

%-----------------------------------------------------------------------------------------------------
\subsection{Задача регрессии белковых структур на~$\text{CAD}_\text{score}$}

Пусть $\mathbf{X} = \bigcup_{i=1}^m \mathbf{x}_i$. Рассмотривается множество параметрических моделей $\mathfrak{F}$, взятых из класса графовых свёрточных нейронных сетей: 
\[\mathfrak{F} = \{\mathbf{f}_k\colon(\mathbf{w}, \mathbf{X})\to  \mathbf{\hat{y}}\mid k \in \mathfrak{K}\},\] 
где $\mathbf{w} \in \mathbb{W}$~-- параметры модели, $\hat{\mathbf{y}} = \mathbf{f} (\mathbf{X},\mathbf{w}) \in \mathbb{R}^{m}$~-- вектор оценок предсказаний CAD-scores. 

Решается задача регрессии для предсказания численного значения $y_i$ $\text{CAD}_\text{score}$ белка на основе его смоделированной пространственной структуры $\mathbf{x}_i$.

Параметры модели $\mathbf{w}\in \mathbb{W}$ минимизируют функцию ошибки на обучении. Определим функцию ошибки:
\[\mathfrak{L}(\mathbf{y}, \mathbf{X}, \mathbf{w}) =\left\lVert \hat{\mathbf{y}} - \mathbf{y} \right\rVert^{2}_2,\]
где $\mathbf{\hat{y}} = \mathbf{f} (\mathbf{X},\mathbf{w})$~-- $\text{CAD}_\text{score}$ предсказанный моделью $\mathbf{f}$, $\mathbf{y}$~-- данный в выборке $\text{CAD}_\text{score}$.
Таким образом, решается данная задача оптимизации: 
$$\textbf{w}^* = \underset{\mathbf{w}\in\mathbb{W}}{\text{argmin}}(\mathfrak{L}(\textbf{w}))$$

Для оценивания качества модели анализируются коэффициенты корреляции Пирсона ($R$), Спирмена ($\rho$)~\cite{3DCNN, Baldassarre2019GraphQAPM, 10.1093/bioinformatics/btz122}. Для каждой нативной структуры белка вычисляются коэффициенты корреляции Пирсона ($R^\text{target}$), Спирмена ($\rho^\text{target}$) между истинными и прогнозируемыми $\text{CAD}_\text{score}$ для смоделированных структур, соответствующих данной нативной структуре белка. Затем коэффициенты корреляции усредняются по всем $T$ нативным структурам. Обозначим $\mathbf{y}_i \in \mathbb{R}^{m_i}$ и $\mathbf{\hat{y}}_i \in \mathbb{R}^{m_i}$ соответственно вектор истинных значений и вектор предсказаний $\text{CAD}_\text{score}$ для смоделированных структур белка, соответствующих нативной структуре $i$. Здесь $m_i$~-- количество смоделированных структур для $i$-ой нативной структуры. Тогда коэффициенты корреляции записываются:
$$\begin{aligned}
R = R\left(\mathbf{y}, \hat{\mathbf{y}}\right) = \frac{1}{T} \sum_{i=1}^{T} R^\text{target}_i=\frac{1}{T} \sum_{i=1}^{T} \text{PEARSON} \left(\mathbf{y}_i,\hat{\mathbf{y}}_i\right) \\ 
\rho= \rho\left(\mathbf{y}, \hat{\mathbf{y}}\right) = \frac{1}{T} \sum_{i=1}^{T} \rho^\text{target}_i = \frac{1}{T} \sum_{i=1}^{T} \text{SPEARMAN} \left(\mathbf{y}_i,\hat{\mathbf{y}}_i\right)
\end{aligned}$$
Здесь $\text{PEARSON} (\cdot, \cdot)$ и $\text{SPEARMAN} (\cdot, \cdot)$~-- корреляции Пирсона и Спирмена соответственно:
\[\text{PEARSON}\left(\mathbf{y}_i,\hat{\mathbf{y}}_i\right) = \frac{\sum_{l=1}^{m_i}\left(\mathbf{y}_{i l}-\bar{\mathbf{y}}_{i}\right)\left( \hat{\mathbf{y}}_{i l}-\bar{ \hat{\mathbf{y}}}_{i}\right)}{\sqrt{\sum_{l=1}^{m_i}\left( \mathbf{y}_{i l}-\bar{ \mathbf{y}}_{i}\right)^{2} \sum_{l=1}^{m_i}\left( \hat{\mathbf{y}}_{i l}-\bar{ \hat{\mathbf{y}}}_{i}\right)^{2}}}\]

\[ \text{SPEARMAN} \left(\mathbf{y}_i,\hat{\mathbf{y}}_i\right)=\frac{\sum_{l=1}^{m_i}\left(\operatorname{rank}\left(
	\mathbf{y}_{i l}\right)-\frac{m_i+1}{2}\right)\left(\operatorname{rank}\left( \hat{\mathbf{y}}_{i l}\right)-\frac{m_i+1}{2}\right)}{\frac{1}{12}\left(m_i^{3}-m_i\right)}\]

%-----------------------------------------------------------------------------------------------------
\subsection{Построение матриц смежности}
Т.к. данные о белках не содержат информации о соединениях между атомами, т.е. нет матрицы смежности, для всех взятых смоделированных структур белков вычисляются матрицы смежности A по следующим правилам:

\begin{itemize}
	\item не соединяются водород с водородом,
	\item атом не соединяется с водородом, если расстояние между ними не менее $1.21$\AA,
	\item не соединяются атомы, которые находятся далеко в последовательности (номера остатков отличаются больше, чем на 1),
	\item не соединяются атомы, создающие дисульфидные связи,
	\item соединяются атомы, расстояние между которыми $r\in \left(r_\text{min}, r_\text{max}\right]$, где $r_\text{min} = 0.01$\AA, $r_\text{max} = \left(0.6\cdot(\rho_\text{atom1}+\rho_\text{atom2})\right)^2$, $\rho_\text{atom}$ -- радиус атома (максимально возможное $r_\text{max} = 5.76$ -- при $\rho_\text{atom1} = \rho_\text{atom2} = 2.0$).
\end{itemize}

По попарным расстояниям между атомами на Рис. \ref{protein_vis} видно, что соединения могут иметь атомы, обозначенные самым светлым желтым, т.к. максимально возможное расстояние между атомами, при котором они могут иметь соединение по представленным правилам составления матрицы смежности равно 5.76. Т.е. матрица смежности будет сильно разреженной.
	\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=0.99\textwidth]{3d_graph.pdf}
		%	\caption{3D визуализация }
		%	\label{fig:data}
	\end{minipage}
	%\hfill
	\begin{minipage}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=0.99\textwidth]{pairwise.pdf}
		%	\caption{Попарные расстояния между элементами белка}
		%	\label{fig:data}
	\end{minipage}
	\caption{Трехмерное представление с помощью координат $\textbf{X}$ и полученной матрицы смежности $\textbf{A}$ и попарные расстояния между атомами смоделированной структуры BAKER-ROSETTASERVER\_TS3 для нативной структуры T0870 из набора данных CASP12}
	\label{protein_vis}
\end{figure}


%-----------------------------------------------------------------------------------------------------
\section{Спектральный анализ}

Для обобщения свёрточных нейронных сетей на графы требуется определить свёрточные фильтры на графах. Существует два подхода: пространственный и спектральный~\cite{DBLP:journals/corr/abs-1901-00596, DBLP:journals/corr/abs-1812-08434}. Как показано в~\cite{ae482107de73461787258f805cf8f4ed}, пространственный подход не имеет общего математического определения трансляции на графах, в то время как спектральный метод имеет хорошее математическое обоснование. Поэтому рассматривается спектральная теория графов.

%\subsection{Представление белковых структур в виде графов}
Элементы аминокислотной последовательности рассматриваются как отдельные узлы, чьи связи (ребра) описывают пространственные отношения между ними. 

В общем случае граф $\mathbf{G}$ определяется набором $\mathbf{(V, A)}$, где $\mathbf{V}\in \mathbb{R}^{n \times c}$ определяет вершины или узлы графа. Матрица смежности $\mathbf{A}\in \mathbb{R}^{n \times n}$ определяет соединения между $n$ узлами (ребра), где $\mathbf{A}_{ij}$~-- наличие связи между узлами $i$ и $j$. Используя это определение графа, белковые структуры можно определить как графы, признаки элементов аминокислотной последовательности которых закодированы в элементах $\mathbf{V}$ узлов, а пространственная близость между элементами закодирована в матрице смежности $\mathbf{A}$.

\subsection{Преобразование графовой свёртки}

\begin{Def}
	\textit{Графовый Лапласиан}~\cite{Chung:1997} -- матрица $\mathbf{L}=\mathbf{I}_{n}-\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}$, где $\mathbf{A}$ -- матрица смежности графа $\mathbf{G}$,  $\mathbf{D}$ -- диагональная матрица степеней вершин, $\mathbf{D}_{i i}=\sum_{j}\left(\mathbf{A}_{i j}\right)$, $\mathbf{I}_{n}$-- единичная матрица.
\end{Def}

Матрица $\mathbf{L}$ является вещественной симметричной положительной полуопределенной, поэтому может быть представлена в виде  $\mathbf{L}=\mathbf{U} \boldsymbol{\Lambda} \mathbf{U}^{\mathsf{T}} $, где $\mathbf{U}=\left[\mathbf{u}_{1}, \mathbf{u}_{2}, \dots, \mathbf{u}_n\right] \in \mathbb{R}^{n \times n}$ -- это матрица собственных векторов, упорядоченных по собственным значениям, $\boldsymbol{\Lambda} \in \mathbb{R}^{n \times n}$~-- диагональная матрица собственных значений (спектр), $\boldsymbol{\Lambda}_{i i}=\lambda_{i}$. Спектральное разложение Лапласиана позволяет определить преобразование Фурье для графов: собственные векторы соответствуют модам Фурье, а собственные значения -- частотам. 

\begin{Def}
	\textit{Графовое преобразование Фурье}~\cite{journals/spm/ShumanNFOV13} для сигнала $\mathbf{x} \in \mathbb{R}^{n}$ задается $\mathscr{F}(\mathbf{x})=\mathbf{U}^{\mathsf{T}} \mathbf{x} \equiv \hat{\mathbf{x}} \in \mathbb{R}^{n}$, а обратное графовое пребразование Фурье: $\mathscr{F}^{-1}(\hat{\mathbf{x}})=\mathbf{U} \hat{\mathbf{x}}$, где $\mathbf{x}$ -- вектор признаков всех вершин.
\end{Def}

Данное преобразование является ключевым в определении графовой свёртки. Оно проецирует входной графовый сигнал на ортонормированное пространство, где базис формируется собственными векторами графового Лапласиана. Элементы преобразованного сигнала $ \hat{\mathbf{x}}$ являются координатами сигнала в новом пространстве, так что входной сигнал может быть представлен как $\mathbf{x}=\sum_{i} \hat{x}_{i} \mathbf{u}_{i}$, что является обратным графовым преобразованием Фурье.

\begin{Th}
	\textbf{(Теорема о свёртках)}~\cite{10.5555/1525499} Преобразование Фурье свёртки двух сигналов является покомпонентным произведением их преобразований Фурье, т.е. $$\mathscr{F}\left( \mathbf{f} * \mathbf{g}\right) =\mathscr{F}(\mathbf{f}) \odot \mathscr{F}(\mathbf{g}).$$
	\label{conv_theorem}
\end{Th}

Следуя из теоремы~\ref{conv_theorem}, спектральная свёртка на графах определяется для сигнала $\mathbf{x}$ и фильтра $\mathbf{g} \in \mathbb{R}^{n}$ как 
\begin{align}
\mathbf{x} * \mathbf{g} &=\mathscr{F}^{-1}(\mathscr{F}(\mathbf{x}) \odot \mathscr{F}(\mathbf{g})) =\mathbf{U}\left(\mathbf{U}^{\mathsf{T}} \mathbf{x} \odot \mathbf{U}^{\mathsf{T}} \mathbf{g}\right) = \mathbf{U g}_{\theta} \mathbf{U}^{\mathsf{T}} \mathbf{x},\label{spec_conv}
\end{align}
где $\mathbf{g}_{\theta} = diag\left(\mathbf{U}^{\mathsf{T}} \mathbf{g}\right)$ -- спектральные коэффициенты фильтра.

Спектральные методы отличаются выбором фильтра $\mathbf{g}_{\theta}$. Соотношение \eqref{spec_conv} вычислительно дорогое, т.к. спектральное разложение требует $O\left(n^{3}\right)$ операций, а перемножение с матрицей собственных векторов $\mathbf{U}$ требует $O\left(n^{2}\right)$ операций. Chebyshev Spectral CNN (ChebNet)~\cite{NIPS2016_6081} обходит эти проблемы аппроксимацией $\mathbf{g}_{\theta}$ с помощью полиномов Чебышева $\mathbf{T}_k\mathbf{(x)}$, убирая необходимость считать собственные векторы Лапласиана $\mathbf{L}$.

\begin{Def}
	\textit{Полиномы Чебышева} $\mathbf{T}_k\mathbf{(x)}$ $k$-ого порядка задаются рекуррентным соотношением  $ \mathbf{T}_{k}(\mathbf{x})=2 \mathbf{x} \cdot \mathbf{T}_{k-1}(\mathbf{x})-\mathbf{T}_{k-2}(\mathbf{x}), \mathbf{T}_{0}(\mathbf{x})=1, \mathbf{T}_{1}(\mathbf{x})=\mathbf{x}$. Образуют ортогональный базис в $L^{2}\left([-1,1], \cfrac{d x} {\sqrt{1-x^{2}}}\right).$
\end{Def}

Представляя $\mathbf{g}_{\theta}$ в виде 
\[\mathbf{g}_{\theta}=\sum_{k=0}^{K} \theta_{k} \mathbf{T}_{k}\mathbf{(\tilde{\Lambda})},	\]
где $\mathbf{\tilde{\Lambda}} = 2 \mathbf{\Lambda} / \lambda_{\max }-\mathbf{I}_{n} \in[-1,1]$, $\lambda_{\max }$ -- максимальное собственное число $\mathbf{L}$, а также замечая, что 
\[
\left(\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{\mathsf{T}}\right)^{k}=\mathbf{U} \mathbf{\Lambda}^{k} \mathbf{U}^{\mathsf{T}}
\]
(собственные векторы образуют ортонормированный базис $\mathbf{U}^{\mathsf{T}}\mathbf{U}=\mathbf{I}$), получаем:
\begin{align}
\mathbf{U g}_{\theta} \mathbf{U}^{\mathsf{T}} \mathbf{x}=\mathbf{U}\left(\sum_{i=0}^{K} \theta_{k} \mathbf{T}_{k}(\tilde{\mathbf{\Lambda}})\right) \mathbf{U}^{\mathsf{T}} \mathbf{x} = \sum_{k=0}^{K} \theta_{k} \mathbf{T}_{k}(\tilde{\mathbf{L}}) \mathbf{x},
\label{cheb_appr}
\end{align}
где $\tilde{\mathbf{L}}=2 \mathbf{L} / \lambda_{\max }-\mathbf{I}_{n}$.

Graph Convolutional Network (GCN)~\cite{kipf_semi-supervised_2017} используют первое приближение ChebNet. Предполагая $\lambda_{\max} \approx 2$ и беря первые 2 слагаемых в сумме ($K=1$), соотношение~\eqref{cheb_appr} упрощается до 
\begin{align}
\mathbf{x} * \mathbf{g} \approx \tilde{\theta}_{0} \mathbf{x}+\tilde{\theta}_{1}\left(\mathbf{L}-\mathbf{I}_{n}\right) \mathbf{x}=\tilde{\theta}_{0} \mathbf{x}-\tilde{\theta}_{1} \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}} \mathbf{x}.
\end{align}
Приняв $\theta = \tilde{\theta}_0  = -\tilde{\theta}_1$, получаем:
\begin{align}
\mathbf{x} * \mathbf{g}  \approx \theta\left(\mathbf{I}_{n}+\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}\right) \mathbf{x}.
\label{conv}
\end{align}
Оператор в скобках может может привести к вычислительной нестабильности и взрыву или затуханию градиентов, т.к. собственные значения данного оператора $\in [0,2]$. Для решения проблемы в~\cite{kipf_semi-supervised_2017} предлагается \textit{трюк перенормировки}: 
\begin{align*}
\mathbf{I}_{n}+\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}} \rightarrow
\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}}, 
\text{ где }
\tilde{\mathbf{A}}=\mathbf{A}+\mathbf{I}_{n},~ \tilde{\mathbf{D}}_{i i}=\sum_{j} \tilde{\mathbf{A}}_{i j}.
\end{align*}
Дан граф $\mathbf{G}$ и матрица с информацией об узлах $\mathbf{X} \in \mathbb{R}^{n \times c}$ ($n$ -- число узлов и $c$ -- число признаков в каждом узле). Исходя из \eqref{conv} и применяя трюк перенормировки, определяется слой свёртки графа:
\begin{align}
\textbf{U}=\tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{X} \mathbf{W},
\label{Z_conv}
\end{align}
где $\mathbf{W} \in \mathbb{R}^{c \times t}$ – матрица параметров свёртки с $t$ фильтрами, а $\textbf{U} \in \mathbb{R}^{n \times t}$~-- выходная матрица. На рисунке~\ref{fig:convlayer} изображена схема свёрточного слоя на основе полученного преобразования.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.75\textwidth]{conv_layer.pdf}
	\caption{Схема свёртки графа с матрицей $\textbf{X}$ размера $n\times c$, $\text{t}$ -- число фильтров в свёртке, FC -- полносвязный слой. Синий прямоугольник~-- выходная матрица размером $n\times t$}
	\label{fig:convlayer}
\end{figure}

%-----------------------------------------------------------------------------------------------------
\subsection{Функция преобразования регрессионной модели}
Архитектура сети составляется по аналогии с моделью GCN~\cite{kipf_semi-supervised_2017}. На основе выражения~\eqref{Z_conv} определяются свёрточные слои (рисунок~\ref{fig:convlayer}). Нелинейная функция~$\sigma$ выбрана ReLu.

Сеть состоит из 3 свёрточных слоёв, макспулинга $\text{pool}$ по вершинам графа и полносвязного слоя $FC$~-- скалярного умножения с вектором $\mathbf{w}_4$. Параметры свёрток $t$ взяты равными 64, 64, 64 соответственно для первого, второго, третьего свёрточных слоёв. На рисунке~\ref{fig:scheme} представлена схема тестируемой в работе нейронной сети.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.99\textwidth]{net.pdf}
	\caption{Схематическое представление архитектуры свёрточной нейронной сети SpectralQA, использованной в данной работе}
	\label{fig:scheme}
\end{figure}

Таким образом, преобразование $f: \mathbf{X} \rightarrow \text{CAD}_\text{score}$ полученной нейросети записывается в виде
$$f = \langle \mathbf{w}_4, \text{DO} \circ \text{pool} \circ \sigma \left(\mathbf{U}_3\right) \circ\sigma\left( \mathbf{U}_2\right)\circ\sigma\left( \mathbf{U}_1\right)\rangle,$$
где $\mathbf{U}_k= \tilde{\mathbf{D}}^{-\frac{1}{2}} \tilde{\mathbf{A}}\tilde{\mathbf{D}}^{-\frac{1}{2}} \mathbf{X} \mathbf{W}_k$, $\text{DO}$~-- дропаут, $\text{pool}$~-- максимум по всем узлам графа.

%-----------------------------------------------------------------------------------------------------
\section{Вычислительный эксперимент}

\subsection{Описание данных}
Данные для эксперимента берутся с соревнований CASP разных лет. Используются наборы данных CASP9--CASP12 (таблица~\ref{table:student}). Данные представляют собой пары нативная-смоделированная структуры, каждая из которых описана координатами и химическими свойствами атомомв структуры. Обучение модели происходит на данных CASP9--CASP11, тестирование~-- на CASP12. Для процессов обучения и тестирования по формуле~\eqref{CAD_score} вычисляются $\text{CAD}_\text{score}$ для всех смоелированных структур на основе нативных структур. 

\begin{table}[H]
		\centering
		\caption{Наборы данных белковых структур}
		\begin{tabular}{p{28mm}|p{26mm}p{26mm}|p{28mm}}
		\hline Набор & Нативные структуры & Модели структур& Разбиение\\
		\hline 
		%CASP 7 & 95 & 24183 & \\
		%CASP 8 & 123 & 36176 &  \multirow{2}{*}{Train,} \\
		CASP 9 & 117 & 35963 &  \multirow{2}{*}{Train,} \\
		CASP 10 & 103 & 15450 & \multirow{2}{*}{Validation} \\
		CASP 11 & 84 & 12291 &  \\
		\hline 
		CASP 12 & 37 & 5501 & {Test} \\
		\hline
		Суммарно & 341 & 69205 &
		\end{tabular}
		\label{table:student}
\end{table}


%-----------------------------------------------------------------------------------------------------
\subsection{Собственное пространство матриц смежности}

Для каждой полученной матрицы смежности $\textbf{A}$ и матрицы после прохождения свёртки $\textbf{U}_k$ производится сингулярное разложения для получения собственных чисел матрицы. На Рис.~\ref{A_eigens} и~\ref{u_k_eigens} представлены собственные числа для смоделированной структуры STRINGS\_TS3, соответствующей нативной T0759.

\begin{figure}[H]
	\begin{minipage}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=1.1\textwidth]{T0759_A_before_conv}
		\caption{Собственные числа $\textbf{A}$}
		\label{A_eigens}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.49\textwidth}
		\centering
		\includegraphics[width=1.1\textwidth]{T0759_X_after_conv}
		\caption{Собственные числа $\textbf{U}_k$}
		\label{u_k_eigens}
	\end{minipage}
\end{figure}

 Для оценки размерности собственного пространства матриц используется правило сломанной трости~\cite{Component_retention}. Набор собственных чисел сравнивается с порогомами: для матрицы $\textbf{A}$ с порогом A, для $\textbf{U}_k$~-- с порогом U. По правилу сломанной трости j-ый собственный вектор $\textbf{A}$ сохраняется в списке главных компонент, если $\lambda_j > A$. Аналогично для $\textbf{U}_k$.
 
Для каждой нативной структуры из данных CASP11 и CASP12 было выбрано случайным образом по одной смоделированной структуре. Для каждой из выбранных смоделированных структур посчитаны собственные числа для матриц $\textbf{A}$ и $\textbf{U}_k$. За размерность собственных пространств матриц взято количество собственных чисел, больших порога. Были рассмотрены пороги $U=10$ и $A \in \{0.5, 1.0, 2.0\}$. 

Результаты представлены на Рис.~\ref{eigens_dims}, на котором каждая точка соответствует одной смоделированной структуре. Размерность собственного пространства матрицы после прохождения через свёртку сжимается в 50-100 раз. Это может быть объяснено сильной разреженностью матриц смежности белковых структур.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.99\textwidth]{eigens.png}
	\caption{Собственные пространства для порогов $U=10$ и $A~\in~\{0.5, 1.0, 2.0\}$.}
	\label{eigens_dims}
\end{figure}


%-----------------------------------------------------------------------------------------------------
\subsection{Анализ корреляций Пирсона и Спирмена}
При обучении нейросети анализируются усредненные по $T$ нативным структурам коэффициенты корреляции Пирсона и Спирмена. Процесс обучения представлен на рисунках~\ref{fig:GCN} и~\ref{fig:correlation}

%\begin{figure}[H]
%	\begin{minipage}[b]{0.54\textwidth}
%		\centering
%		\includegraphics[width=1.1\textwidth]{training.pdf}
%		\caption{График MSE ошибки на обучающей и тестовой выборке}
%		\label{fig:GCN}
%	\end{minipage}
%	\hfill
%	\begin{minipage}[b]{0.45\textwidth}
%		\centering
%		\includegraphics[width=0.99\textwidth]{training_correlations_temp.pdf}
%		\caption{Корреляция Пирсона и Спирмена при обучении}
%		\label{fig:correlation}
%	\end{minipage}
%\end{figure}

\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{training_smallset.pdf}
		\caption{График MSE ошибки SpectralQA на обучающей и тестовой выборке}
		\label{fig:GCN}
	\end{figure}
Графики корреляций Пирсона и Спирмена стабилизируются возле одного значения (рисунок~\ref{fig:correlation}). Большая дисперсия объясняется тем, что для некоторых нативных структур сложно смоделировать структуру, из-за чего $\text{CAD}_\text{score}$ будет равным 0 для плохих смоделированных структур в силу выражения~\eqref{CAD_score}, а не близким к нулю значением. Этим же и объясняется невысокое значение корреляции.
\begin{figure}[h]
		\centering
		\includegraphics[width=0.65\textwidth]{training_correlations_smallsettt.pdf}
		\caption{Корреляция Пирсона и Спирмена при обучении}
		\label{fig:correlation}
\end{figure}

В таблице~\ref{Tab:results} представлены результаты тестирования модели на данных соревнования CASP12. Корреляция здесь берется между всеми предсказаниями и истинными значениями, а не как при обучении усредненная по нативным структурам. Из сравнения данных в таблице видно, что модель из данной работы дает качество, сравнимое с качеством альтернативных моделей, дающих наилучшее качество в задаче.
\begin{center}
	\begin{table}[H]
		\centering
		\caption{Сравнение корреляции Пирсона и Спирмена существующих современных алгоритмов с моделью SpectralQA на данных CASP12}
		%\begin{tabular}{l|l|l|l}
		\begin{tabular}{ccc}
			\hline Метод & Spearmann $\rho$ &  Pearson $R$ \\
			\hline ProQ3D & 0.801 & 0.750 \\
			VoroMQA & 0.803 & 0.766  \\
			SBROD & 0.685 & 0.762  \\
			Ornate & \textbf{0.828} & \textbf{0.781}   \\
			\textbf{SpectralQA} (данная работа) &   {0.746}&   0.647   \\
			\hline 
		\end{tabular}
		\label{Tab:results}
	\end{table}
\end{center}

%-----------------------------------------------------------------------------------------------------
\newpage
\section*{Заключение}
\addcontentsline{toc}{section}{\protect\numberline{}Заключение}

Предложено решение задачи оценки качества прогнозирования структуры белка с использованием графовых сверток. Проведен анализ графовых свёрток на данной задаче. Проведен анализ корреляций Пирсона и Спирмена предсказаний полученной модели и истинных значений качества структуры на данных CASP12. Качество, достигаемое моделью, сравнимо с качеством альтернативных моделей, дающих наилучшее качество в задаче. В дальнейших исследованиях предлагается в основе архитектуры сети использовать другие существующие улучшения спектральных свёрток (CayleyNet, Adaptive Graph Convolution Network). Также предлагается учитывать в данных дополнительные химические свойства атомов и в матрице смежности учитывать не только наличие связи, но и расстояния между атомами при наличии связи.

\newpage

%-----------------------------------------------------------------------------------------------------
\newpage
\addcontentsline{toc}{section}{\protect\numberline{}Список литературы}
\bibliographystyle{ugost2008}
\bibliography{references}
\nocite{*}







\end{document} 